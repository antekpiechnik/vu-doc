\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage[pdftex]{graphicx}
\usepackage{subfigure}


\begin{document}
\title{\textbf{Profiling users media preferences based on social network data streams}}
\author{Konrad Delong \and Antoni Piechnik}
\date{June 27 2010}

\maketitle

\begin{abstract} Social networks have been becoming hugely popular over the last
years. With enormous number of people sharing information about their lives,
they are now a great source of information about their media interests. Based on both
structured data (such as the social aspect of YouTube) as well as unstructured
(Twitter), we analyze what data can be extracted from their profiles as well as
how useful such extraction is for profiling those users. Such profiling could
prove hugely influential for media-oriented recommender systems, such as the one
used in the \textit{NoTube}\footnote[1]{http://www.notube.tv} project.
We focus on aggregating and analyzing the publicly available data and covering
different approaches of profile generation
for users. Our experiments reveal multiple ways of employing social networks'
data for users profiling as well as show promising results for possibly employing
this data in a real-life project.
\end{abstract}

\section{Introduction}

The NoTube project bases its recommendation of TV programmes on aggregating, extracting and analyzing user activities. Since media oriented web services (such as YouTube) hold a great amount of information regarding a users' video viewing preferences, it might provide a great deal of additional information for profiling NoTube users. General activity services (social networks like \textit{Facebook or Twitter}) detail the lives of its users, it is very likely that they also contain information regarding their TV watching activities.

The question is \textit{what user data we can collect from both structured and unstructured social applications?} By \textit{structure} of a social application, we understand the structure of user's input into the social application. An example of a \textit{structured} ''input'' would best be depicted by a user clicking a ''Like'' button under a YouTube video (leaving straightforward information regarding his preference)
whereas a single 140 character activity information update on Twitter (that may consist of virtually anything) would be considered \textit{unstructured}.

When it comes to YouTube, users can mark videos as favorite, subscribe to other users' channels and comment on videos. From all that information we can not only extract the titles of videos they are interested in but also tags describing them as well as relations to other videos. On the other hand, unstructured data streams like Twitter contain much more irrelevant information, but can enable us to extract a greater amount of activities limited only by what the natural language offers. Apart from extracting the activities, we will be able to relate the entities (like TV programme and channel names). A user profile could also be partially generated from a user's activity frequency as well as the users they are following and the hash tags used.

We will also describe ways \textit{of measuring the accuracy and effectiveness of profiling a user based on those social data streams}. We would like to find out how efficient browsing through different networks is when it comes to generating those profiles, both in terms of the quality of information regarding the media preferences as well as its amount relative to the amount of data collected. We will attempt to find a perfect ratio between the level of detail of the information we extract and the quality of user profiles generated. We will evaluate interest counting strategies for different types of entities recognized in various contexts.

In the second section of this document we will describe the structure of the data available on the mentioned social services (both \textit{Facebook} and \textit{Twitter}) along with an analysis of which of their parts might be suitable for our research. The following section will describe the \textit{Freebase.com} linked data service that we used and it's data's nature. Sections 4 and 5 include descriptions of our approaches for extracting data from \textit{YouTube} and \textit{Twitter}. The next section consists of an evaluation of our results as well as a comparative analysis of both services. This section will be followed by a discussion of achieved results. Last but not we will cover the implementation details.

\subsection{Related work}

A considerable number of papers examining social web services have been
published. Works devoted to YouTube often contain analysis of the video data
stored by the service and its implications to the traffic generated
(\cite{i-tube-you-tube}, \cite{views-from-the-edge},
\cite{statistics-and-social-network}), some study impact of YouTube service on
very narrow topics, like 2006 USA presidential elections
\cite{voters-myspace-youtube}, or social attitude towards vaccinacions
\cite{keelan}. There are also papers analyzing privacy issues of using YouTube
\cite{publicly-private}.

However, none of these works give focus to user-profile generation. In our
paper, we will analyze usefulness of data from two social services in generating
user profiles.

Apart from the papers mentioned, we have also found various implementations of systems that aggregate personal preferences based on different
social services:

\subsection{Hunch.com}
\textit{Hunch.com} is a service offering recommendations on different topics based on a user's twitter data. It predicts preferences in forms of small questions and is then able to recommend a product or a solution to a problem that is represented with a \textit{Topic} on the site. \textit{Hunch.com} seems to be more commercial-oriented, e.g. providing users with links to online stores with products they are recommending.

\paragraph{Approach}
The application can create recommendations based solely on user's twitter data (mostly people they follow). However, when not that much data is available, it requires input from users in order to make future predictions more accurate. It also seems to be covering topics a bit broader than the NoTube project.
\textit{Hunch.com} does not provide information on how the recommender system works, although it provides an API for accessing their data.
Hunch is able to adapt to much more different topics, and it not necessarily focuses on their semantics (rather on what similar people simply like).

\subsection{Open Graph}
\textit{Open Graph} has been introduced by Facebook in order to create a system for stating preferences by clicking a \textit{Like} button displayed on different pages. Due to the popularity of Facebook, it certainly will become a major source of data for any kind of recommender systems.

\paragraph{Approach}
\textit{Open Graph} provides developers with an API for accessing it's data. It is restricted by a given user's privacy settings.
It aggregates information based on the sites the user clicks the button on, thus being able to gather a great deal of information. \textit{Open Graph} does not seem to be also approaching any kind of Twitter data, making our Twitter profiling methods a completely separate topic.
The biggest disadvantage is no \textit{Dislike} button, making it impossible for a user to state negative opinion on a given subject.

%\include{introduction}

\include{data_sources}

\include{vocabularies}

\include{youtube_extracting}

\include{twitter_extracting}

\include{results}

\include{evaluation}

\include{discussion}

\section{Implementation details}

\subsection{Technologies used in Twitter research}
\subsubsection{Language}
Most processing has been done using the \textit{Ruby} language. The NoTube tubelets/reasonlets will be implemented in either pure \textit{Java} or \textit{JRuby}.
\subsubsection{Tools}
Twitter API requests are been handled via the OAuth protocol and provided in a JSON-based stream form. Only the newest tweets for the pre-selected Twitter usernames are downloaded. The entities definitions and semantics are provided by the FreeBase database REST interface. All the processing occurs locally on the pre-loaded Twitter data. \\ Ruby gems used include:
\begin{itemize}
  \item \textit{bdb} from mattbauer - a BerkeleyDB wrapper for Ruby
  \item \textit{twitter} from jnunemaker - a wrapper around the Twitter API
  \item \textit{ferret} - full text search engine written in Ruby
  \item \textit{ken} from michael-aufreiter - wrapper for the freebase.org API
\end{itemize}
\subsubsection{Storage}
Due to Twitter API request rate limiting, tweets have been aggregated in a local
\textit{Berkeley DB} database instance providing a fast and easy key-value
information retrieval. Since great amount of data needs to be processed, a full
text search engine has been used, which greatly increased the searching time. In
this case, we decided on using \textit{ferret} - a Ruby implementation of a
full-text search engine (similar to Apache's \textit{Lucene}).

\bibliographystyle{plain}
\bibliography{sources}

\end{document}
