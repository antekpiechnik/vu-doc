\section{Results evaluation}

In this chapter we would like to evaluate the results of our profiling approaches as well as attempt to compare both streams and their usefulness for the NoTube project.

\subsection{Comparative analysis of YouTube and Twitter as sources of user
profiling data}

\subsubsection{Access to and the structure of data}
The greatest issue with generating user profiles is the availability of the data. This covers not only access to remotely
stored data, but also how information on user preferences is stored within it.
\\ The Twitter data source is challenging to process due to very restrictive API Access Limit Rates, making closed applications
unable to work at full speed. On the other hand, YouTube has a mostly complete (except for e.g. user's history) public API,
access to which is virtually unrestricted and unlimited for researching systems. This is a great advantage, allowing to use
much more approaches and aggregate more data for a greater deal of users.

\subsubsection{Type of data extractable}
Due to great diversity of data stored in a Twitter users' streams, only a little it's part can be effectively used for profiling
users. That forces any kind of research to start with locating useful data. This however is not much different from YouTube, where
a lot of users have data almost completely inadequate for any kind of research.
\\ Furthermore, the language of streams is also a factor, since it is difficult to extract preferences from Non-English Twitter
streams. The same problem applies to YouTube, despite it's API being already structured. A great deal of tags is English,
but this may also vary depending on both user's and video's language, hence it is useful to detect their language before
proceeding any further. Moreover in Twitter, analyzing an update's usefulness, any script will need to apply text searching (mostly
unnecessary in YouTube), which also makes the whole process slower.
\\ Yet the greatest difference between those two sources is the internal structure (or lack thereof) of atomic information
available. YouTube has all of it's data perfectly organized (such as tags for videos, tags for channels, channels users are
subscribed to, etc.) and revolving around videos, whereas on Twitter, a single user can post any kind of text that is
less or equal than 140 characters in length, making extraction of specific preference data nontrivial and possibly inaccurate and forcing to abandon traditional
NLP methods
Since all tweets are limited to 140characters, they are usually formed as incorrect sentences,
\subsubsection{Usefulness for the NoTube project}
Both services contain large amounts of information useful for recommender
systems, like the one designed for NoTube project. For both of them, the
efficiency for a single user depends mainly on user's activity, and amount of
data generated. YouTube videos offer several ways of identifying their content.
They are also frequently linked from other services. For Twitter, virtually single
way of examining content is through text analysis. On the other hand, Twitter
messages are actually generated by the user who is being profiled. Most of the data
on YouTube comes from other publishers. In other words: after seeing an
interesting television show user is likely to post a Twitter update saying ''I
enjoyed watching X'', while it is rather unlikely that she will search for a
video related to that movie somehow.

YouTube also exposes additional data useful for user profiling: gender, age and
location. In the \textit{cold start} problem, those pieces of information are very
often used to narrow down the initial set of recommended items. Twitter is much
more minimalistic in this area -- only email address, full name and the login
name are required for registration. However, it is worth noting, that all
mentioned pieces of information are commonly faked by the users wishing to keep
their privacy.

\subsection{Evaluation of user profiling by users}
\subsubsection{Measuring efficiency of user profiling}

Two ways of measuring efficiency were employed. The first one, was splitting the
input set into training and test data. The algorithm was run on training data,
and results were compared with the testing set. Since testing data set had the
same structure as the training set, this technique was equivalent to running the
same algorithm twice and comparing the results.

This technique was used for analysis of Twitter data. For data imported from
YouTube, this approach was not applicable because of its scarcity. Instead,
manual verification was performed. A survey was performed over a group of 20
people. The survey contained list of topics extracted from user's profile mixed
with random unrelated subjects. Each topic was assigned a mark ranging from 1 to
5 which was supposed to reflect user's interests in particular subject.
