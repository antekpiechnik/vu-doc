\section{Results evaluation}

\subsection{Comparative analysis of YouTube and Twitter as sources of user
profiling data}

\subsubsection{Data availability}
Twitter
- unstructured
- hard to obtain
- limited API

YouTube
- public/unlimited API
- more users

\subsubsection{Type of data extractable}
Twitter
- completely unstructured/mostly irrelevant for the research
- hard obtaining accurate preferences, only interest/activity
- taking slower (text search/indexing)
- hardy applicable NLP methods

YouTube
- mostly language-independent
- structured
- API parts describing preferences (?)

\subsubsection{Usefulness for the NoTube project}

Both services contain large amounts of information useful for recommender
systems, like the one designed for NoTube project. For both of them, the
efficiency for a single user depends mainly on user's activity, and amount of
data generated. YouTube videos offer several ways of identifying their content.
They are also frequently linked from other services. For Twitter, virtually single
way of examining content is through text analysis. On the other hand, Twitter
messages are actually generated by the user who is being profiled. Most of the data
on YouTube comes from other publishers. In other words: after seeing an
interesting television show user is likely to post a Twitter update saying ''I
enjoyed watching X'', while it is rather unlikely that she will search for a
video related to that movie somehow.

YouTube also exposes additional data useful for user profiling: gender, age and
location. In the ``cold start`` problem, those pieces of information are very
often used to narrow down the initial set of recommended items. Twitter is much
more minimalistic in this area -- only email address, full name and the login
name are required for registration. However, it is worth noting, that all
mentioned pieces of information are commonly faked by the users wishing to keep
their privacy.

\subsection{Evaluation of user profiling by users}
\subsubsection{Measuring efficiency of user profiling}

Two ways of measuring efficiency were employed. The first one, was splitting the
input set into training and test data. The algorithm was run on training data,
and results were compared with the testing set. Since testing data set had the
same structure as the training set, this technique was equivalent to running the
same algorithm twice and comparing the results.

This technique was used for analysis of Twitter data. For data imported from
YouTube, this approach was not applicable because of its scarcity. Instead,
manual verification was performed. A survey was performed over a group of 20
people. The survey contained list of topics extracted from user's profile mixed
with random unrelated subjects. Each topic was assigned a mark ranging from 1 to
5 which was supposed to reflect user's interests in particular subject.
