\section{Results evaluation}

\subsection{Comparative analysis of YouTube and Twitter as sources of user
profiling data}

\subsubsection{Data availability}
Twitter
- unstructured
- hard to obtain
- limited API

YouTube
- public/unlimited API
- more users

\subsubsection{Type of data extractable}
Twitter
- completely unstructured/mostly irrelevant for the research
- hard obtaining accurate preferences, only interest/activity
- taking slower (text search/indexing)
- hardy applicable NLP methods

YouTube
- mostly language-independent
- structured
- API parts describing preferences (?)

\subsubsection{Usefulness for the NoTube project}
Twitter
- depending on user's activity, might actually contain more information (describing media activity in TV and Theatres)
- profiles way less accurate than YouTube

YouTube
- ?? (more short-movies, music and stuff)

\subsection{Evaluation of user profiling by users}
\subsubsection{Measuring efficiency of user profiling}

Two ways of measuring efficiency were employed. The first one, was splitting the
input set into training and test data. The algorithm was run on training data,
and results were compared with the testing set. Since testing data set had the
same structure as the training set, this technique was equivalent to running the
same algorithm twice and comparing the results.

This technique was used for analysis of Twitter data. For data imported from
YouTube, this approach was not applicable because of its scarcity. Instead,
manual verification was performed. A survey was performed over a group of 20
people. The survey contained list of topics extracted from user's profile mixed
with random unrelated subjects. Each topic was assigned a mark ranging from 1 to
5 which was supposed to reflect user's interests in particular subject.
