\section{User evaluation}
\label{sec:evaluation}

To evaluate the quality of our results, we performed user surveys
asking users to judge the extracted profiles. Two groups of users were chosen,
each counting about 15 persons, most of them active users of \textit{Twitter} and \textit{YouTube}.
These users have been selected from friends of authors in order to be able to easily
retrieve results of the accuracy of profiling. The surveys have been conducted via
e-mail.

\subsection{Surveys}

Each person was presented a list of media-related topics (TV shows and actors) 
that were extracted from their online
activity and asked to mark their level of interest in every one of
them. The marks ranged from 0 (dislike) to 3 (strongly like).
Such scale was suggested for users to be able to quickly respond
and yet provide us with enough data to measure the accuracy of the prediction.
This set was extended with additional mark ''X'', meaning
''I never heard of this'', which let us quickly identify false positives that
sneaked into extracted profiles.

Such a scale, in spite of being simple, let us capture three basic attitudes
towards a given subject: positive, neutral and negative. Only the positive
attitude was further divided into two levels so that a person could express a
more enthusiastic feelings. The simplicity of the scale also turned the filling
of a survey into a very straight-forward process.

The results of the survey were collected and compared to the generated profiles.
Each ''X'' mark in the survey meant that an item shouldn't have been matched,
and thus should be considered a false positive. The rate of these missed is
presented in section 6.3. Among the rest of the results, we analyzed how big
part of the marks we estimated were the same as the ones provided by the user.
The results of this comparison are presented in section 6.4.

\subsection{Overview of the survey results}
The activeness of users fit in relatively wide range: from 20 up to 400 tweets,
and 6-400 favourites. On average, \textit{2.51\%} of tweets were media-related.
One of the \textit{Twitter} users has noted a score of \textit{18\%} of his
tweets containing mentions of media references. For YouTube, average rate of
videos with NEs detected was \textit{9.56\%} of the analyzed set.

\subsection{False positives in the survey results}
The number of false positives was large. For some users, this
number reached more than 50\% -- maximum of 73.37\% for \textit{Twitter} using the most accurate method (we discuss
the method accuracy in section \ref{sec:false_positives_evaluation}) and 83\% for \textit{YouTube}.
This score makes the use of any recommender algorithm, that is based on such a profile, challenging.
Luckily, such degenerate (more than 50\% false positives) cases were relatively uncommon (1 and 2 in accordingly for
Twitter and YouTube surveys) and happened for users with small numbers of items in their streams.
We have also experimented with methods of removing false positives from the generated user profile,
as described below.

\subsubsection{Removing false positives}
\label{sec:false_positives_evaluation}

We have analyzed the effect of different methods of minimizing the amount of false
positives for Twitter profiling (as described in Section 6.3) by providing test users
polls with results of different methods. The results are presented in the table below:

\begin{center}
  \begin{tabular}{ | p{2.5cm} | p{2cm} | p{2.5cm} | p{2cm} | p{2cm} | } \hline
    False Positives removal & Method & False Positives removal applied to & Tweets with NEs & Incorrectly identified interests \\ \hline
    \multirow{2}{*} {No}
      & FN & N/A & 3.11\% & 92.17\% \\ \cline{2-5}
      & FN and HT & N/A & 3.62\% & 80.81\% \\ \cline{2-5}
    \hline
    \multirow{2}{*} {Yes}
      & FN and HT & FN and HT & 1.63\% & 67.25\% \\ \cline{2-5}
      & FN and HT & FN only & 1.64\% & 61.13\% \\ \cline{2-5}
    \hline
  \end{tabular} \\
  Table 20: Results of different matches filtering methods on the generated user profile.
\end{center}

Table 20 contains the results of using the false positives removal methods. In the Figure,
\textit{FN} stands for Full Name, \textit{HT} stands for \textit{hashtag}, where \textit{False Positives removal}
column states whether \textit{false positives} removal methods have been applied.
The \textit{Tweets with NEs} column shows the amount of matched NEs within all tweets, whereas
the \textit{False positives} column shows the percentage of falsely matched
NEs within all NEs matched.

For this experiment we have used the \textit{British National Corpus}\footnote{http://www.natcorp.ox.ac.uk/} --
a 100 million word collection of samples of written and spoken language from a wide range of sources. We have
decided to employ this service due to its accessibility, ease of use and the quality of data it provides.
The table shows that the number false positives without any matching provides us with many matches, less than
10\% being correct. By incorporating \textit{hashtag} searching, we have received more results and decreased the amount
false positives. After applying corpus filtering, the amount of NEs matched has decreased over twice in size,
but we have also noted a decrease in the amount of false positives. Applying the text corpus ranking only to
NEs matched by full name allowed us to further decrease the false positives percentage.
This further suggests that \textit{hashtags} tend to contain more accurate data regarding user preferences.

The corpora ranking also has its downsides. For one of the test users, the corpus ranking method helped to
decrease the false positives by from 74\% to 48\%, but also removed two (out of 11) \textit{accurate} matches (marked as
accurate in the non-filtered polls -- a Movie \textit{The Weekend} and a TV Show called \textit{I'm telling}).
However, since the accuracy is the most important aspect of user profiling, employing this method is worth
the risk.

\subsection{Number of ratings guessed correctly}
\label{sec:evaluation_ratings_correct}
After counting out false positives, the correctness of ratings was judged. The
number of rates guessed correctly by the Twitter profiler revolving around \textit{70\%},
while only \textit{25\%} of YouTube rate guesses turned out to be correct. This
result seems surprising when compared to rates of NEs found in both
services. Our data suggests that Twitter profile, even though containing less
items, is of a higher quality than the one based on YouTube data.  This
might suggest that the frequency of tweeting on a given subject as well as
preference vocabulary used is a better indicator of interest than a number of
videos in user's YouTube favourites feed, despite using simple methods of rating
the predicted preference.

\paragraph{Comparing multiple user streams in YouTube}

The multiple user streams from YouTube have been compared for their usefulness
for their adequacy in extracting the user's interests. The results are presented
in Table 21. Unfortunately, the users from our survey did not have enough
uploads to measure this video set appropriately. For that reason, the uploads
set is missing from the table. We can see, however, that the favourites set
tends to not only contain more correct matches, but also was more likely to
reflect the user rating correctly (through the use of occurrence counting).

\begin{center}
  \begin{tabular}{| l | l | l |}
  Video set & False positives & Correct matches \\ \hline
  Favourites & 33\% & 25\% \\
  Subscriptions & 80\% & 0\% \\
  \end{tabular} \\
  Table 21: Comparison of subscriptions and favourites
\end{center}
