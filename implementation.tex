\section{Implementation details}
\label{sec:implementation}

This section covers the code that performed most of the experiments used for the research in Sections 2, 5 and 6.

\subsection{Twitter Experiments}
Most processing has been done using the \textit{Ruby}\footnote{http://www.ruby-lang.org/en/}
language, a dynamic, scripting open source programming language. This allowed us
to focus on experimenting instead of implementation. The profiler scripts have been
separated from any native extensions, making them compatible with \textit{JRuby}\footnote{http://www.jruby.org}
-- the Java implementation of the Ruby language.

\subsubsection{Structure}
The \textbf{Extractor} part retrieves the data from Twitter. The Twitter API requests are being handled via the
\textit{OAuth}\footnote{http://oauth.net/} protocol and provided in a
\textit{JSON}-based stream form. Only the newest tweets for the Twitter
users are downloaded. The NE definitions and semantics are provided by
the \textit{Freebase} database REST interface. All further processing by the \textit{Associator}
occurs locally on the pre-loaded Twitter data.

\subsubsection{Workflow}

The \textit{Twitter} data is accessed by the \textit{Extractor} via the \textit{Twitter}
API\footnote{http://apiwiki.twitter.com/}, which provides data as a collection of \textit{JSON} objects. It provides
information on followers of a user as well as contains a collection of his tweets. This data is then being
stored in an instance of \textit{BerkeleyDB}\footnote{http://www.oracle.com/technology/products/berkeley-db/index.html}
under a key representing the user uniquely. The NEs for processing are extracted from the \textit{Freebase}
dataset using the \textit{Freebase} HTTP API\footnote{http://www.freebase.com/docs/acre\_api} also in
form of \textit{JSON} Objects. The most important information (names and \textit{Freebase} IDs) are then
saved in \textit{YAML}\footnote{http://www.yaml.org/} files. All this data is then combined and used
for experiments by the \textit{Associator}. It uses the power of the \textit{Ruby} language for processing tweets
of a user using methods described in sections \ref{sec:ner} and \ref{sec:user_activity} (mostly using \textit{regular
expressions}). When generating a user profile, the Associator outputs a YAML file containing user's interests and
their weights. This workflow is shown on Figure 2. We have also implemented scripts for generating a text file
containing surveys used in section \ref{sec:evaluation}, basing on user profile generated.

\begin{center}
  \includegraphics[scale=0.65]{images/twitter_diagram.pdf} \\
  Figure 2: The structure of the Twitter data experiments workflow
\end{center}

\subsubsection{Tools}
Apart from using the standard Ruby libraries, separate open-source Ruby gems have been used for the experiment:
\begin{itemize}
  \item \textit{bdb}\footnote{http://github.com/mattbauer/bdb} - a wrapper for the BerkeleyDB C driver
  \item \textit{twitter}\footnote{http://github.com/jnunemaker/twitter} - a wrapper around the Twitter API
  \item \textit{ferret} \footnote{http://ferret.davebalmain.com} - full text search engine written in pure Ruby
  \item \textit{ken}\footnote{http://github.com/michael/ken} - a wrapper for the freebase.org API
\end{itemize}

\subsubsection{Storage}
Due to Twitter API request rate limiting, tweets have been aggregated in a local
\textit{Berkeley DB} database instance providing a fast and easy key-value
information retrieval. Since a huge amount of data needs to be processed, a full
text search engine has been used, which greatly increased the searching time. In
this case, we decided on using \textit{ferret} - a full-text search engine
written in Ruby (similar to Apache's \textit{Lucene}).

\subsection{YouTube Experiments}
The code used for the research was written in \textit{Python}\footnote{http://www.python.org}
programming language. Using high-level programming language let us focus on the code itself, and skip the
gory details like memory management.

\subsubsection{Structure and Workflow}
The main program -- the one responsible for generating user's preferences
-- connects to \textit{YouTube} API in order to receive the required data. The
API responses come as xml documents, and are analyzed in search of the pieces of
data we are interested in (titles and descriptions). Among these pieces of text,
the Freebase concepts are sought. For repeating items, their number is kept,
which is later used as a preference indicator.  The Freebase data
is stored as flat files, downloaded from the dumps regularly published by the
Freebase service.

The corpora for analysis was collected using breadth-first search graph
traversal through YouTube contacts. The corpus of video titles and descriptions
was collected based on those user's favourites.

The corpus contains little over 100,000 videos and is 36Mbytes in size. This
data was used to compute number of video descriptions with identifiable concepts from
\textit{Freebase}\footnote{http://freebase.com} data vocabularies (more on this in section 4).

\begin{figure}[h!]
  \begin{center}
	  \includegraphics[scale=0.65]{images/youtube_diagram.pdf} \\
	  Figure 3: The structure of the YouTube data experiments workflow
  \end{center}
\end{figure}

\subsubsection{Tools}

The code for user profiling depends solely on modules contained in the standard
library, which simplifies the installation process and lets us avoid dependency
on a particular Python implementation (such an approach makes the eventual
switch to \textit{Jython}\footnote{http://www.jython.org} far easier). The
standard library modules used were: ElementTree -- for xml processing, csv --
for reading \textit{Freebase} data dumps and urllib2 -- for accessing the API.

A set of additional scripts was prepared that were used for YouTube data
analysis. These scripts rather than analyzing single user's data, worked over a
large number of users in order to gather information on how YouTube is used.
This information includes number of people using various features (see section
\ref{sec:uad_youtube_concepts}), rates of videos with identifiable linked data concepts
(section \ref{sec:evaluation_overview}).
Parts of data extracted for this research were only possible to obtain after
authentication. This was performed using http-based Google's protocol designed
for this purpose.
