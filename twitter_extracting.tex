\section{Using unstructured user data from Twitter}

In this section, we will evaluate the use of entity reference extractions
mentioned in Section 3.2 when ran against the \textit{Twitter} corpus (as described in Section 3.2.1).
We will discuss the results and their usefulness for generating a media preference profile of a user.
We will also cover any problems encountered and suggest possible solutions.

\subsection{Approach}
Since the \textit{Twitter} corpus has been selected as described in the Section 3.2.1,
we decided to cross-validate the measurements against two randomly selected halves of the corpus
to be able to minimize the overfitting of the data. By the measurements, we understand methods
of entity reference extraction described in Section 3.2.The results in the following sections suggest
that entity references are evenly distributed between the two parts of the corpus.

We have used vocabularies consisting of \textit{TV Actors, TV Programmes, Movies and Movie Actors} from
the \textit{Freebase} dataset.

Initial results show that using simple string matching methods we are able to extract interests from
a user's Twitter stream. For an active Twitter user, it holds that up to \textit{18\%} of their tweets
contain mentions of media entities (more on that in section 8.2).

However, there is a great number of entity names (mostly TV Shows) that create noise in the results, \eg show titles such as \textit{Me too}. Removing those false positives has a crucial effect on the eventual accuracy of a Twitter-based profiler
(as we shall see in section 8.3.1).

\subsection{Results}
In this section we present results of locating references to entities using different methods (as described in Section 3.2).
In each paragraph we present results for both halves of the corpus, which we will refer to as \textit{Corpus 1} and
\textit{Corpus 2}.

\subsubsection{Detecting mentioned entities}
\paragraph{Full name matching}
We have measured the average ratio of tweets with entities found to all tweets for different kinds of entities using full name matching of the entity name.

\begin{center}
  \begin{tabular}{ | p{4cm} | p{2cm} | p{1cm}| p{2cm} | } \hline
    Entity (average) & Corpus 1 & & Corpus 2 \\ \hline
    TV Shows & 62.54\% & & 77.21\% \\ \hline
    TV Actors & 0.41\% & & 0.00\% \\ \hline
    Movies & 30.81\% & & 26.19\% \\ \hline
    Movie actors & 0.74\% & & 0.21\% \\ \hline
  \end{tabular} \\
  Table 8: Percentage of all tweets with recognized entities using full name matching \\
\end{center}

Scores presented in Table 8 are used as the a baseline below. We can easily notice high numbers for TV Shows and
Movie entities. As we noticed in our results, most of those matches have been accidental when the entity name is a single
word highly popular in natural language (such as \textit{Love} or \textit{Hotel}. We will refer to those accidental
matches as \textit{false positives} and will discuss avoiding them in section 6.3. Out of half of the corpus, an average of 99 tweets contained a reference to an actor (TV or Movie). This is a low score, but considering the purpose of \textit{Twitter} (as described in section 3.2), we have been expecting them and still believe this amount might be enough for user profiling. On the other hand, the number of such tweets for TV show names is almost 14,500 tweets. This suggests
that \textit{false positives} problem will be a obstacle to achieve accurate results.

\paragraph{Matching twitter usernames}
Due to small number of available Twitter usernames for entities in the \textit{Freebase} dataset (as of writing this paper -- 182 \textit{Twitter} usernames related to TV), this approach is not proving effective. After including those usernames in the search,
we noticed little (0.02\% in on of the corpuses for TV Shows) to almost none increase in the results.

\paragraph{Matching title acronyms}
Only entities with a title of 3 or more words have been used for this measurement, since searching
for smaller acronyms has generated a great amount of noise in the results. We have omitted Actors names, because
they mostly consist of two words and their acronyms rarely correspond to those actors.

\begin{center}
  \begin{tabular}{ | p{4cm} | p{2cm} | p{1cm}| p{2cm} | } \hline
    Entity (average) & Corpus 1 & & Corpus 2 \\ \hline
    TV Shows & 3.07\% & & 2.17\% \\ \hline
    Movies & 2.01\% & & 2.33\% \\ \hline
  \end{tabular} \\
  Table 8: Percentage of all tweets with recognized entities using the entity's name acronym \\
\end{center}

We can easily spot an increase in matched entities, since acronyms such as \textit{SNL}
(for \textit{Saturday Night Live} show) or \textit{BBT} (\textit{Big Bang Theory}) are widely used.
However there might be many misleading acronyms created with this approach. If an acronym is similar
to a natural language word (\eg \textit{CAT}), it will drastically increase the noise and the amount
of \textit{false positives} generated by the matching algorithms (thus the higher percentage of tweets
found). This problem might be solved by methods for avoiding the false positives, which we discuss in section 6.3.

\paragraph{Matching of the name converted to a hashtag form}
For this measurement, all entities' names have been converted to a \textit{hashtag} form (as described
in Section 3.2). In this experiment, also one-word entity names have been used.

\begin{center}
  \begin{tabular}{ | p{4cm} | p{2cm} | p{1cm}| p{2cm} | } \hline
    Entity (average) & Corpus 1 & & Corpus 2 \\ \hline
    TV Shows & 1.18\% & & 2.21 \% \\ \hline
    TV Actors & 0.00\% & & 0.03\% \\ \hline
    Movies & 2.49\% & & 3.12\% \\ \hline
    Movie actors & 0.67\% & & 0.09\% \\ \hline
  \end{tabular} \\
  Table 10: Percentage of all tweets with recognized entities using hashtag formed from the entity name \\
\end{center}

The results in Table 10 let us notice how the person-based entities have either noted a smaller
occurrence rate and the title-based ones have improved. This may be related to the
fact that people usually create hashtags based on a more general entity (such as
a movie) rather then it's specific parts (such as actors that play in it)
when expressing their opinion \cite{edinburg-corpus}

\paragraph{Measuring the frequency of entities in matched tweets}
We have decided to measure how much entities can be found in an average matched tweet. By a matched tweet,
we understand a tweet where a mention of an entity has been found (via any of the methods mentioned above).

\begin{center}
  \begin{tabular}{ | p{4cm} | p{2cm} | } \hline
    Method & Ratio \\ \hline
    Full name & 1.69 \\ \hline
    Acronyms & 1.02 \\ \hline
    Hashtags & 1.13 \\ \hline
  \end{tabular} \\
  Table 11: The average amount of entities found within a tweet \\
\end{center}

The results in Table 11 show that on an average, every time a tweet contains a mention of an entity,
it is likely to contain an additional reference to some other entity. This suggests that users try to add
more references to a tweet when it's already related to media. This might also be helpful for detecting the
context of a single tweet in search of media references.


\subsubsection{Detecting opinion on an entity}
\paragraph{Usage of activity verbs}
For this experiment, a rather small activity verbs vocabulary has been used. (verbs such
as \textit{to watch, to play, to see, to check out, to catch} with their past forms.
\\ Below is a figure showing percentage of tweets using an activity verb
and a entity name (full match) out of all that have been matched with an
entity's name (hence the higher scores).

\begin{center}
  \begin{tabular}{ | p{4cm} | p{2cm} | p{1cm}| p{2cm} | } \hline
    Entity (average) & Corpus 1 & & Corpus 2 \\ \hline
    Movies & 37.4\% & & 24.3\% \\ \hline
    TV Shows & 21.2\% & & 13.4\% \\ \hline
    TV Actors & 1.3\% & & 0.6\% \\ \hline
    Movie actors & 2.1\% & & 0.0\% \\ \hline
  \end{tabular} \\
  Table 12: Average ratio of tweets with activity verbs found to all tweets with entity references. \\
\end{center}

As we can see, the activity verbs are very unlikely to be occurring next to
person-based entities. However, activity verbs are much more popular with both
Movies and TV Shows, which originates from the very idea of Twitter for
updating statuses with information on what the user is currently doing (such as \textit{watching}
a movie).

\paragraph{Usage of preference vocabulary}
Two sets of preference have been used:
\begin{itemize}
  \item positive -- such as \textit{like, recommend, love, great, awesome, stunning, good}
  \item negative -- such as \textit{hate, bad, worse, poor}
\end{itemize}

The Table 13 below shows the general use of preference verbs with occurrences of
mentions.

\begin{center}
  \begin{tabular}{ | p{4cm} | p{2cm} | p{1cm}| p{2cm} | } \hline
    Entity (average) & Corpus 1 & & Corpus 2 \\ \hline
    TV Shows & 7.4\% & & 6.3\% \\ \hline
    Movies & 9.1\% & & 8.4\% \\ \hline
    TV Actors & 0.0\% & & 0.9\% \\ \hline
    Movie actors & 1.2\% & & 2.7\% \\ \hline
  \end{tabular} \\
  Table 13: Average ratio of tweets with preference verbs found to all tweets with entity references \\
\end{center}

We have also measured the positive-to-negative preference ratio (Table 14):

\begin{center}
  \begin{tabular}{ | p{3cm}| p{2cm} | } \hline
    Type & Amount \\ \hline
    Positive & 92\% \\ \hline
    Negative & 8\% \\ \hline
  \end{tabular} \\
  Table 14: The amount of positive and negative verbs found within the tweets with entity references found \\
\end{center}

The amount of preference verbs used whilst mentioning an entity is definitely
smaller compared to activity verbs. However, the positive-to-negative ratio suggests that users'
media preferences expressed on Twitter are mostly positive.

\paragraph{Preference towards entities followed by a user}
By gathering available Twitter usernames from the \textit{Freebase} database,
we were able to perform searches for mentions of those usernames within the followers' streams.

The Table 15 shows the share of different kind of mentions of the specific entity while following.

\begin{center}
  \begin{tabular}{ | p{3cm}| p{2cm} | } \hline
    Match type & Occurence \\ \hline
    Name & 0\% \\ \hline
    Hashtag & 8\% (2 occurrences) \\ \hline
    Username & 92\% (22 occurences) \\ \hline
  \end{tabular} \\
  Table 15: The amount of mentions by different methods of the entity being followed by a user in his stream \\
\end{center}

Since Twitter usernames mostly represent specific people rather than any other kinds of entities, it seems as if users mostly
mention them using their usernames rather than their names in plain or hashtag form.

However, those mentions occur relatively rarely. For a sample of 70 twitter usernames and 5 followers each
(around 4000 tweets), we were able to find out only 24 tweets (about 0.57\%) mentioning directly the people they are following.

Furthermore, following a certain entity should also be regarded just as a preference toward the topics this entity is related to (such as Politics for \textit{BarackObama}). On the other hand, automated locating of more \textit{official} twitter usernames for various entities is challenging, which
limits the use of this approach.

\subsection{Avoiding false positives}
By \textit{false positives}, we understand matches that have been falsely assigned to a user stating user's interest.
They occur usually when a user uses the entity name in a unrelated context when the entity name is similar to a
part of natural language.

In order to reduce the chance of such accidental name matches occurring, entity names were ran against a corpus
of English texts, ranking them by their frequency. The more often an entity name occurs in those corpora, the smaller the chance of it occurring as a name of the TV shows rather than a natural language expression. In order to rate the certainty, a separate function is introduced, taking into account the following:

\begin{itemize}
  \item form of occurrence of the entity name (string match, hashtag)
  \item use of an activity verb
  \item use of a preference verb
  \item the frequency of the entity name in a collection of samples of written and spoken language
\end{itemize}

We expect the first three of those factors to have a huge influence on the accuracy of the entity recognition within a tweet (basing on results in section 6.2). The can see in section 8.3.1 that such approach provided great results for
reducing the amount of false positives.

This measurement is easily translatable into the preference weights to be recorded when profiling a user.
Moreover, by undertaking semantic approaches we are able to analyze different topics found in a tweet and
attempt to rank their relation to the entity we are researching. We will discuss this in the section 9 (Future work).

