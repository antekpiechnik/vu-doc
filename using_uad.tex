\section{Using user activity data}
\label{sec:usage_uad}

Our object is to extract a user profile. The goal of this section is to create a comparative analysis the UAD
available on of different \textit{Twitter} and \textit{YouTube} identified in section \ref{sec:uad} for generating
a user profile. We focus on discussing methods and approaches. We also describe encountered challenges and discuss
proposed solutions.

\subsection{Method}
In this section we want to present methods and results of using the UAD described in section \ref{sec:uad} for
generating a user profile. We will focus on the Named Entity Recognition, as well as extracting relevant types
of user activity for both services. We expect many \textit{false positives} to occur, \ie incorrectly identified NEs
-- when the NE name is a single word highly popular in natural language (such as \textit{Love} or \textit{Hotel}).
We will refer to those accidental matches as \textit{false positives} and will discuss avoiding them in section
\ref{sec:false_positives}.

\subsubsection{Named Entity Recognition}
\label{sec:ner}

\paragraph{Identifying named entities in Youtube streams}
The linked data concepts can be associated with user data through the use of
simple text search. The video's title and description fields often carry names
of people depicted in the clip. Having a set of all actors names we can
identify their occurrences.

This approach is not ideal. Many \textit{false positives} can be returned, particularly
for concepts that are also common phrases (\eg the 'Lost' TV show).

\paragraph{Pervasiveness of linked data concepts in YouTube}
A set of statistics was prepare to measure the pervasiveness of linked data
concepts as extracted from the \textit{Freebase} vocabularies. The matching performed
was a simple text-based search in videos titles and descriptions. It's results
are shown in the Table 8.

\begin{figure}[h!]
  \begin{center}
    \begin{tabular}{ | p{4cm} | p{6cm} | } \hline
      Entity (average) & Rate of videos with matched contents 1 \\ \hline
      TV Shows & 30.25\% \\ \hline
      TV Actors & 14.73\% \\ \hline
      Movies & 45.95\% \\ \hline
      Movie actors & 13.12\% \\ \hline
    \end{tabular} \\
    Table 8: Rate of video descriptions with NEs found using using full name matching. \\
  \end{center}
\end{figure}

These numbers are exceptionally large, but unfortunately, a vast number of text
based matches proved to be false positives. This was the case particularly for
movie titles, where NEs with common labels are quite popular (\eg 'Under',
'House', or 'Power').

\paragraph{Full name matching of named entities in Twitter}
We have measured the average ratio of tweets with NEs found to all tweets for different kinds of NEs
using full name matching of the NE label.

\begin{center}
  \begin{tabular}{ | p{4cm} | p{2cm} | p{1cm}| p{2cm} | } \hline
    Entity (average) & Corpus 1 & & Corpus 2 \\ \hline
    TV Shows & 62.54\% & & 77.21\% \\ \hline
    TV Actors & 0.41\% & & 0.00\% \\ \hline
    Movies & 30.81\% & & 26.19\% \\ \hline
    Movie actors & 0.74\% & & 0.21\% \\ \hline
  \end{tabular} \\
  Table 9: Percentage of all tweets with recognized NEs using full name matching \\
\end{center}

Scores presented in Table 9 are used as the a baseline below. Given the results in Table 9, we believe
most of them are \textit{false positives}. Out of half of the corpus, an
average of 99 tweets contained a reference to an actor (TV or Movie). This is a low score, but considering the
purpose of \textit{Twitter} (as described in section \ref{sec:twitter_uad}), we have been expecting them and still believe this amount
might be enough for user profiling. On the other hand, the number of such tweets for TV show names is almost 14,500
tweets. This suggests that \textit{false positives} problem will be a obstacle to achieve accurate results.

\paragraph{Matching Twitter usernames}
Due to small number of available Twitter usernames for NEs in the \textit{Freebase} dataset (as of writing this
paper -- 182 \textit{Twitter} usernames related to TV), this approach is not proving effective. After including those
usernames in the search, we noticed little (0.02\% in on of the corpuses for TV Shows) to almost no increase in
the results.

\paragraph{Matching title acronyms in Twitter}
Only NEs with a title of 3 or more words have been used for this measurement, since searching
for smaller acronyms has generated a great amount of noise in the results. We have omitted Actors names, because
they mostly consist of two words and their acronyms rarely correspond to those actors. The results
are shown in Table 10.

\begin{center}
  \begin{tabular}{ | p{4cm} | p{2cm} | p{1cm}| p{2cm} | } \hline
    Entity (average) & Corpus 1 & & Corpus 2 \\ \hline
    TV Shows & 3.07\% & & 2.17\% \\ \hline
    Movies & 2.01\% & & 2.33\% \\ \hline
  \end{tabular} \\
  Table 10: Percentage of all tweets with recognized NEs using the NE's label acronym \\
\end{center}

We can easily spot an increase in matched NEs, since acronyms such as \textit{SNL}
(for \textit{Saturday Night Live} show) or \textit{BBT} (\textit{Big Bang Theory}) are widely used.
However there might be many misleading acronyms created with this approach. If an acronym is similar
to a natural language word (\eg \textit{CAT}), it will drastically increase the noise and the amount
of \textit{false positives} generated by the matching algorithms (thus the higher percentage of tweets
found). This problem might be partially solved by methods for avoiding the false positives,
which we discuss in section \ref{sec:false_positives}. Case sensitive matching could also be helpful,
but due to time constraints we were unable to perform enough experiments.

\paragraph{Matching of the name converted to a hashtag form (Twitter)}
For this measurement, all NEs labels have been converted to a \textit{hashtag} form (as described
in Section 3.2). In this experiment, also one-word NE labels have been used.

\begin{center}
  \begin{tabular}{ | p{4cm} | p{2cm} | p{1cm}| p{2cm} | } \hline
    Entity (average) & Corpus 1 & & Corpus 2 \\ \hline
    TV Shows & 1.18\% & & 2.21 \% \\ \hline
    TV Actors & 0.00\% & & 0.03\% \\ \hline
    Movies & 2.49\% & & 3.12\% \\ \hline
    Movie actors & 0.67\% & & 0.09\% \\ \hline
  \end{tabular} \\
  Table 11: Percentage of all tweets with recognized NEs in hashtag form \\
\end{center}

The results in Table 11 let us notice how the person-based NEs have either noted a smaller
occurrence rate and the title-based ones have improved. This may be related to the
fact that people usually create hashtags based on a more general entity (such as
a movie) rather then its specific parts (such as actors that play in it)
when expressing their opinion \cite{edinburg-corpus}

\paragraph{Measuring the frequency of NEs in matched tweets}
We have decided to measure the average amount of detected NEs per matched tweet. By a matched tweet,
we understand a tweet where a mention of an NE has been found (via any of the methods mentioned above).

\begin{center}
  \begin{tabular}{ | p{4cm} | p{2cm} | } \hline
    Method & Ratio \\ \hline
    Full name & 1.69 \\ \hline
    Acronyms & 1.02 \\ \hline
    Hashtags & 1.13 \\ \hline
  \end{tabular} \\
  Table 12: The average amount of NEs found within a tweet \\
\end{center}

The results in Table 12 show that on an average, every time a tweet contains a mention of an NE,
it is likely to contain an additional reference to some other NE. This suggests that users try to add
more references to a tweet when it's already related to media. This allows us to consider the context
in which NEs are used for improving the accuracy of the Named Entity Recognition methods.

\paragraph{Comparison of Named Entity Recognition in YouTube and Twitter}
By comparing results from Tables 8 and 9, \textit{YouTube} looks to be a more promising area for this
kind of search. The linked data concepts were present in up to 50 times as many items as for the other
web service for every type of concepts searched. This is most probably a result of different natures of
text data in both services.

\textit{Twitter} web service doesn't encourage its users to publish their tweets in any
specific form. The only requirement is that the length of a single message
is less than 140 characters, making them too short for users to elaborate
on details of their activities. Moreover, tweets are often parts of
conversations. In a video description on \textit{YouTube}, even the name
suggests that this field should be used to characterize the contents
of a video. Such a difference in suggested use might explain the fact that
many more NEs were found in \textit{YouTube} stream. Also, a relatively
unrestrained form of tweets is probably the reason for a lower rate of
concepts found in data coming from this web service. Often, instead of referring to
a NE by its full name, \textit{Twitter} users often use a person's
last name or the acronym of a TV show. This results in a lower effectiveness of
text-based NE finder.

\subsubsection{Extracting relevant types of user activity}

\paragraph{Exploring user-video relation} Even after a video is enriched by
concepts from Linked Open Data, user's attitude towards it should by analyzed.
For Twitter, the context of a NE is searched for verbs that would indicate
level of interest. For YouTube, such approach is futile as the user in most
cases has no impact on description of the video. What we can do, however, is
analyze the structure of data, that we work with. The presence of a video in
user's favourites, subscriptions or uploads set suggests some relation between
the two. We can narrow down the nature of that relation by checking which set
specifically the video belongs to.

The upload stream is generated by the user herself, so presence of an entity in
there should suggest a strong interest. Lack of presence, however, should not
disqualify an interest: user does not need to be publish videos about things
and people she likes. As our research shows, for most users, this set had least
entities found.

On the other hand, presence in the subscription videos is much less an
indicator of an interest -- it is not the user, who publishes videos in this
set, and the set might contain elements, that are ignored by the user keeping
the subscription for other items. To verify these assumptions, we performed a
set of experiments using data from these two sets.

\paragraph{Usage of activity verbs in Twitter}
For this experiment, a small set of activity verbs has been used: \textit{watch, watching, watched,
play, playing, played, checking out, saw, seen}.

Table 13 shows the ratio of tweets that contain both a NE and an activity verb over all
tweets that contain a NE.

\begin{center}
  \begin{tabular}{ | p{4cm} | p{2cm} | p{1cm}| p{2cm} | } \hline
    Entity (average) & Corpus 1 & & Corpus 2 \\ \hline
    Movies & 37.4\% & & 24.3\% \\ \hline
    TV Shows & 21.2\% & & 13.4\% \\ \hline
    TV Actors & 1.3\% & & 0.6\% \\ \hline
    Movie actors & 2.1\% & & 0.0\% \\ \hline
  \end{tabular} \\
  Table 13: Average ratio of tweets with activity verbs found to all tweets with NE references. \\
\end{center}

As we can see, the activity verbs are very unlikely to be occurring next to
person-based NEs. However, activity verbs are much more popular with both
Movies and TV Shows, which originates from the very idea of Twitter for
updating statuses with information on what the user is currently doing (such as \textit{watching}
a movie).

\paragraph{Usage of opinion vocabulary in Twitter}
Two sets of opinion words have been used:
\begin{itemize}
  \item positive -- \textit{like, recommend, love, great, awesome, stunning, good}
  \item negative -- \textit{hate, bad, worse, poor}
\end{itemize}

The Table 14 below shows the general use of opinion words with occurrences of
mentions. We have also measured the positive-to-negative preference ratio (in Table 15).

\begin{center}
  \begin{tabular}{ | p{4cm} | p{2cm} | p{1cm}| p{2cm} | } \hline
    Entity (average) & Corpus 1 & & Corpus 2 \\ \hline
    TV Shows & 7.4\% & & 6.3\% \\ \hline
    Movies & 9.1\% & & 8.4\% \\ \hline
    TV Actors & 0.0\% & & 0.9\% \\ \hline
    Movie actors & 1.2\% & & 2.7\% \\ \hline
  \end{tabular} \\
  Table 14: Average ratio of tweets with opinion words found to all tweets with NE references \\
\end{center}

\begin{center}
  \begin{tabular}{ | p{3cm}| p{2cm} | } \hline
    Type & Amount \\ \hline
    Positive & 92\% \\ \hline
    Negative & 8\% \\ \hline
  \end{tabular} \\
  Table 15: The amount of positive and negative verbs found within the tweets with NE references found \\
\end{center}

The amount of preference verbs used whilst mentioning a NE is definitely
smaller compared to activity verbs. The positive-to-negative ratio suggests that users'
media preferences expressed on Twitter are mostly positive.

\paragraph{Preference towards NEs followed by a user in Twitter}
We gathered available Twitter usernames from the \textit{Freebase} database and performed searches
for mentions of those usernames within the followers' streams.

The Table 16 shows the share of different kind of mentions of the specific NE while following.

\begin{center}
  \begin{tabular}{ | p{3cm}| p{3.5cm} | } \hline
    Match type & Occurence \\ \hline
    Name & 0\% \\ \hline
    Hashtag & 8\% (2 occurrences) \\ \hline
    Username & 92\% (22 occurences) \\ \hline
  \end{tabular} \\
  Table 16: The amount of mentions by different methods of the NE being followed by a user in his stream \\
\end{center}

Since Twitter usernames mostly represent specific people rather than any other kinds of NEs, it seems as if users mostly
mention them using their usernames rather than their names in plain or hashtag form.

However, those mentions occur relatively rarely. For a sample of 70 \textit{Twitter} usernames and 5 followers each
(around 4000 tweets), we were able to find out only 24 tweets (about 0.57\%) mentioning directly the people they are
following. However, such little amount of data is not enough to base conclusions on.

Furthermore, following a certain NE should also be regarded just as a preference toward the topics this NE is related to
(such as Politics for \textit{BarackObama}). On the other hand, automated locating of more \textit{official}
\textit{Twitter} usernames for various NEs is challenging, which limits the use of this approach.

\paragraph{Comparison of relevant user activities extraction}

The interest extraction, even though based on a similar technique (simple text
search), has different meaning for Twitter and YouTube data.  Concepts from
YouTube video streams have clearly defined role (favourite, upload or
subscription) whereas Twitter updates can refer to a NE in a way (positive,
negative, neutral) that needs to be extracted as well with a possibility of
introducing errors in the process. However, if extracted correctly (finding a
preference verb in a Twitter update), the detected interest is valuable,
since it describes user's relation with the item directly. For YouTube, on the
other hand, the information is determined, but its meaning might differ (we
cannot determine the level of interest based only on the presence or a lack
thereof). YouTube streams also tend to contain more references to media-related
items than Twitter updates. 

This diversity (more information of low quality for YouTube and less
information of high quality from Twitter) would make the profile combined from
both sources more robust.

\subsection{Challenges}

During our experiments we have encountered various challenges. In this section, we would like to describe
the most significant ones as well as present and discuss possible solutions.

\subsubsection{Multiple streams of user activity}

While the Twitter data contains a single UAD stream only, the data gathered from
YouTube is divided into several streams of different natures. This causes
several problems, but also is a source of unexpected benefits. This section
describes these challenges and explains why it does not concern Twitter's UAD.

\paragraph{Duplicates in YouTube}

When combining tags collected from various sets of videos, a question should be
asked what preference, should be given to tags appearing in more
then one of them. Table 17 presents three examples of favourites,
subscriptions and uploads intersections. The letters $f$, $s$ and $u$ mean
numbers of videos in accordingly: favourites, subscriptions and uploads. The
combinations of letters indicate the size of the intersections of those sets. \\

\begin{center}
\begin{tabular}{| l | l | l | l | l | l | l | l |}
user & $f$ & $s$ & $u$ & $fs$ & $fu$ & $su$ & $fsu$ \\ \hline
politician & 219 & 32321 & 357 & 116 & 59 & 156 & 34 \\
metal music fan & 1103 & 7570 & 73 & 533 & 27 & 44 & 26 \\
one of authors & 687 & 5553 & 10 & 199 & 2 & 2 & 1 \\
\label{intersections}
\end{tabular} \\
Table 17: Sizes of intersecting favourite, subscription and upload sets of sample users. \\
\end{center}

The tags that appeared in all three sets are:
\begin{itemize}
  \item{politician: \emph{wybory, europarlament, czerwca, europa, europejska,
  parlament, 18, unia, europejski, kraków, w, 7, pe, 2009, bogusław, po,
  bezpieczeństwo}}
  \item{metal music fan: \emph{head, death, pantera, for, in, nosturi, the, live, cover, metal}}
  \item{one of authors: \emph{film}}
\end{itemize}

As expected, the tags in the intersection of sets give much more precise view of person's
interests as compared to tags occurring in a single set. Tags get muted when a user is not active in one of
those areas (\eg does not upload movies very frequently). In order to minimize its effect on the
end result, weights for different sets may be used, and we can maximize weights for tags occurring
in multiple sets.

\paragraph{Combining tags of large and small sets}
Gathering tags across user's favourites or uploads let us treat the whole video
set as a single document, giving each tag equal weight in the counting process.
Taking a similar approach for subscriptions (\eg giving equal weight to each
tag of each video of every subscription) leads to some unwanted effects.

Suppose a user subscribes to two channels. One represents the British royal
family (133 uploads), the other one: Apple Inc., an American computer company
(19 uploads). The tags referring to new hardware releases would get flooded with
tags regarding british-specific content. In effect, the user's profile would
depend not on user's choice of subscriptions, but rather on how often new videos
are published on these channels.

In order to counteract this effect a number of techniques can be used. One of
them is giving tags in each subscription a weight, based on the number of videos or
total number of tags. These kinds of techniques can lead to another distortion of
the final results, giving preference to subscriptions with a few videos.
During experiments, the authors were not able to find a normalization technique
that gave fundamentally better results.

\paragraph{Comparing tags and categories}

Analyzing tags information carries some problems. Tags of a single video clip
can be either too numerous, making many
of them irrelevant, too sparse, nonexistent and occasionally just wrong. To avoid
that problem, we aggregate the tags across sets of video clips related to the
profiled user (favourites, subscriptions, uploads). Similar aggregation was
performed for the videos' categories. Categories are similar to tags (in that:
each video has many categories and many tags applied). They are chosen,
however, from an extremely limited (12 items) set
of options.  Below is a comparison of results of counting
tags (chosen by the user) and categories (picked from a small predefined set).

\begin{table}[ht]
\begin{minipage}[b]{0.5\linewidth}\centering

\begin{tabular}{| l | l |}
Category & \# \\ \hline
News & 8 \\
Music & 5 \\
Education & 3 \\
Comedy & 2 \\
\end{tabular}

\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.5\linewidth}
\centering

\begin{tabular}{| l | l |}
Tag & \# \\ \hline
sonik & 5 \\
bogusław & 5 \\
trzech & 5 \\
kupli & 5 \\
wybory & 4 \\
kraków & 4 \\
polska & 4 \\
\end{tabular}

\end{minipage}

\caption{Most popular tags and categories from account of Bogusław Sonik -- a
polish politician. The word ''wybory'' stands for ''election'' in polish, and
\emph{Trzech kumpli} is a title of a controversial political documentary.}
\end{table}


\begin{table}[ht]
\begin{minipage}[b]{0.5\linewidth}\centering

\begin{tabular}{| l | l |}
Category & \# \\ \hline
Music & 53 \\
Comedy & 20 \\
Entertainment & 8 \\
Games & 4 \\
People & 3 \\
Travel & 3 \\
News & 2 \\
\end{tabular}

\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.5\linewidth}
\centering

\begin{tabular}{| l | l |}
Tag & \# \\ \hline
metal & 17 \\
music & 11 \\
rock & 11 \\
video & 7 \\
the & 7 \\
black & 7 \\
of & 7 \\
\end{tabular}

\end{minipage}

\caption{Most popular tags and categories from anonymous user -- apparently a
heavy metal music fan}
\end{table}

We can see that category count, even though providing general overview of
person's interests (and the ways she uses \textit{YouTube}), cannot be used to find
specific interests. We can say that a person is more keen to
favourite music videos or comedy videos after looking at his categories
(because categories like ''Music'' and ''Comedy'' would stand out), but in
order to say what \textit{kind} of music the person prefers, we need more specific
information gathered from \eg tags. However, for the task of identifying Open Linked
Data entities, both tags and categories can play at most a support role and was
not used for the entity detection in our work.

\paragraph{Multiple streams in Twitter}
The problems caused by multiple streams do not concern the \textit{Twitter} UAD. Each user has a single
data stream on which all extraction is performed. This allowed us to focus more attention
on problems described in section 5.2.2 in relation to \textit{Twitter}.

\subsubsection{Dealing with false positives in the user profile}
\label{sec:false_positives}
By \textit{false positives}, we understand matches that have been falsely assigned to a user stating user's interest.
They occur when NE label is ambiguous, \eg 'Lost'.

\paragraph{Detecting false positives in Twitter UAD}

In order to reduce the chance of such accidental name matches occurring, NE labels were ran against a corpus
of English texts, ranking them by their frequency. The more often an NE label occurs in those corpora, the smaller the chance of it occurring as a name of the TV shows rather than a natural language expression. In order to rate the certainty, a separate function is introduced, taking into account the following:

\begin{itemize}
  \item form of occurrence of the NE label (string match, hashtag)
  \item use of an activity verb
  \item use of a preference verb
  \item the frequency of the NE label in a collection of samples of written and spoken language
\end{itemize}

We expect the first three of those factors to have a huge influence on the accuracy of the Named Entity Recognition within
a tweet, basing on results in section \ref{sec:ner}. We expect adding the last factor to avoid many \textit{false
positives}. We can see in section \ref{sec:false_positives_evaluation} that such approach provided great results for
reducing the amount of \textit{false positives}.

This measurement is easily translatable into the preference weights to be recorded when profiling a user.
Moreover, by undertaking semantic approaches we are able to analyze different topics found in a tweet and
attempt to rank their relation to the NE we are researching. We will discuss this in the section \ref{sec:future}
(Future work).

The methods presented have proved highly effective. As we will discuss in section \ref{sec:false_positives_evaluation},
using the natural text--based corpora can reduce the amount of \textit{false positives} by up to 33\%.

\paragraph{Detecting false positives in YouTube UAD}

A similar technique might be used YouTube research as well. It would drastically
reduce the number of false positives, at the cost, however, of missing the NEs
that actually do have common, single word titles.

Another approach is accepting NE detection only, when it is found in both title
and description of the clip. While such approach has a huge potential, when it
comes to increasing precision of the search, it might refuse too many matches,
as concepts are not always mentioned by name in both title and description of
the video.

\subsection{Generating the User Profile}

For this paper, we define profile as a collection of linked data concepts and weights,
that represent user's interests. Many YouTube clips can be
enriched by linking them to elements presented in the video. For example, a
video with a specific actor can be linked to that actor's freebase id, making it
a part of Linked Open Data. Luckily, for many clips, such a connection can be
detected in an automated manner, using a simple text search.
In addition to that, analysis of tags can give an overview of user's interests. This kind of data is
useful in two ways: it lets humans easily identify user's interests and it can be
used to verify interests generated by other methods (for example the tag set
of user interests might be compared to tags assigned to a movie in another web
service).

\paragraph{Analysing profilers on sample users}
In order to analyze the effectiveness of our profilers, we ran them on groups of
users selected from Twitter and YouTube services. The samples were selected
using two different methods. For Twitter, the website's search service was used
in order to identify users that are likely to post updates about media content.
Such an approach let us skip laborious analysis of accounts with hardly any
updates at all, or updates not related to media content in any manner (which,
given Twitter's access limits, was a considerable gain). For YouTube -- service
with no access limits and no user-by-favourites-search function -- a \textit{breadth-first search}
graph crawl was performed in order to collect the user set.

\paragraph{Generating User Profile from YouTube UAD}

In order to generate a YouTube user profile, we have used the entity
recognition method described in section 5.1. We count the NEs detected in
profiled user's activity streams and combine them with arbitrary weights applied
(1 for favourites, 2 for uploads and 0.5 for subscriptions). The results are
normalized to reflect the scale used for the surveys for user evaluation (see section 6).
The Table 18
presents a part of an example of user profile generated. The other types of
entities are not shown because of the space restrictions.

\begin{center}
  \begin{tabular}{| l | l |}
Named Entity & Rating \\ \hline
Bob Fosse & 1 \\
The Rock & 1 \\
Jack White & 2 \\
John Williams & 2 \\
Leonard Cohen & 1 \\
Bob Hope & 1 \\
Andy Kaufman & 1 \\
Johnny Lee & 2 \\
Crispin Glover & 2 \\
Steven Spielberg & 1 \\
Screamin' Jay Hawkins & 1 \\
John Lennon & 2 \\
The Game & 1 \\
The Enigma & 1 \\
Elvis Presley & 3 \\
Bob Zmuda & 1 \\
Dudley Moore & 2 \\
  \end{tabular} \\
  Table 18: Example user profile (the actors part)
\end{center}

\paragraph{Generating User Profile in Twitter}

For generating user profile from the \textit{Twitter} UAD, we have used methods
described in section \ref{sec:usage_uad}. We have aggregated the occurrences
of mentions of different NEs, giving a higher weight to more precise mentions, \eg
full name mentions having a lower weight than \textit{hashtags}. After
generating the user profile containing all the NEs and preference weights, we have
applied methods for \textit{false positives} removal to NEs that have not achieved
a predefined threshold.

This approach has its downsides. We have applied methods described in section \ref{sec:false_positives}
to the final preference weight, that is without taking into the consideration what
this weight has been based on. By applying those methods while performing the Named Entity Recognition,
we expect to achieve more accurate results. However, this was not possible due to time restrictions.

\paragraph{Analysis of the generated user profiles}

For each user, a profile was generated using our profilers for both Twitter and
YouTube services. We measured effectiveness of profilers as a ratio of number of
items (tweets/videos) with linked data concepts identified to the overall number
of items analyzed. The results are presented in the table below.

\begin{center}
  \begin{tabular}{| l | l | l |}
  Topic & Twitter & YouTube \\ \hline
  Film actors & 0.48\% & 13.12\% \\
  Movie titles & 1.79\% & 45.95\% \\
  TV actors & 0.20\% & 14.73\% \\
  TV shows & 1.5\% & 30.25\% \\
  \end{tabular} \\
  Table 19: The comparison of the amount of linked data concepts within separate data streams \\
\end{center}

The results in Table 19 show that the analyzed \textit{YouTube} UAD contained more linked
data concepts. The reader should note, there was no checking of actual correctness of those results at
this stage (the evaluation of results using surveys   is presented in the next chapter).

\subsection{Variety across concept types}
Another remark worth making is that number of matched items depends on type of
concepts that we are in search of. However, after manual analysis of identified
items, we came to a conclusion that comparing occurrences of
different concepts in data streams is counterproductive at this stage.
That is because of the numbers of false positives, which vary wildly
depending on the topic. This observation will be extended in the next chapter.

\subsection{Dependency on the amount and quality of data}

The effectiveness of the profiler -- not surprisingly -- depends strictly on data
that the profiler was fed. Only a small number of users -- those, whose updates
are published frequently and often feature media-related subjects -- could be
profiled thoroughly. Profiles generated from small amounts of data are considered
to be only stubs, which would be of relatively little use for recommenders. A technique that
might cushion the effect of small amounts of data would be combining results
coming from various web services. This way, in face of scarce YouTube data, the
profiler might still return meaningful results based on data coming from
Twitter or other services, as suggested by \cite{public-profiles}.
